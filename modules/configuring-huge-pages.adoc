// Module included in the following assemblies:
//
// * scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed-by-apps.adoc
// * post_installation_configuration/node-tasks.adoc

:_content-type: PROCEDURE
[id="configuring-huge-pages_{context}"]
= 配置巨页

节点必须预先分配在 统信容器云管理平台 集群中使用的巨页。保留巨页的方法有两种： 在引导时和在运行时。在引导时进行保留会增加成功的可能性，因为内存还没有很大的碎片。Node Tuning Operator 目前支持在特定节点上分配巨页。

== 在引导时

.流程

要减少节点重启的情况，请按照以下步骤顺序进行操作：

. 通过标签标记所有需要相同巨页设置的节点。
+
[source,terminal]
----
$ oc label node <node_using_hugepages> node-role.kubernetes.io/worker-hp=
----

. 创建一个包含以下内容的文件，并把它命名为 `hugepages_tuning.yaml`：
+
[source,yaml]
----
apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: hugepages <1>
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile: <2>
  - data: |
      [main]
      summary=Boot time configuration for hugepages
      include=openshift-node
      [bootloader]
      cmdline_openshift_node_hugepages=hugepagesz=2M hugepages=50 <3>
    name: openshift-node-hugepages

  recommend:
  - machineConfigLabels: <4>
      machineconfiguration.openshift.io/role: "worker-hp"
    priority: 30
    profile: openshift-node-hugepages
----
<1> 将 Tuned 资源的 `name` 设置为 `hugepages`。
<2> 将 `profile` 部分设置为分配巨页。
<3> 请注意，参数顺序是非常重要的，因为有些平台支持各种大小的巨页。
<4> 启用基于机器配置池的匹配。

. 创建 Tuned `hugepages` 对象
+
[source,terminal]
----
$ oc create -f hugepages-tuned-boottime.yaml
----

. 创建一个带有以下内容的文件，并把它命名为 `hugepages-mcp.yaml`：
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-hp
  labels:
    worker-hp: ""
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-hp]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-hp: ""
----

. 创建机器配置池：
+
[source,terminal]
----
$ oc create -f hugepages-mcp.yaml
----

因为有足够的非碎片内存，`worker-hp` 机器配置池中的所有节点现在都应分配 50 个 2Mi 巨页。

[source,terminal]
----
$ oc get node <node_using_hugepages> -o jsonpath="{.status.allocatable.hugepages-2Mi}"
100Mi
----

ifndef::openshift-origin[]
[警告]
====
目前，这个功能只在 UnionTech Enterprise Linux Uswift（Uswift）8.x worker 节点上被支持。在 UnionTech Enterprise Linux（UOS20）7.x worker 节点上，目前不支持 TuneD [bootloader] 插件。
====
endif::openshift-origin[]

////
For run-time allocation, kubelet changes are needed, see BZ1819719.
== At run time

.Procedure

. Label the node so that the Node Tuning Operator knows on which node to apply the tuned profile, which describes how many huge pages should be allocated:
+
[source,terminal]
----
$ oc label node <node_using_hugepages> hugepages=true
----

. Create a file with the following content and name it `hugepages-tuned-runtime.yaml`:
+
[source,yaml]
----
apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: hugepages <1>
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile: <2>
  - data: |
      [main]
      summary=Run time configuration for hugepages
      include=openshift-node
      [vm]
      transparent_hugepages=never
      [sysfs]
      /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages=50
    name: node-hugepages

  recommend:
  - match: <3>
    - label: hugepages
    priority: 30
    profile: node-hugepages
----
<1> Set the `name` of the Tuned resource to `hugepages`.
<2> Set the `profile` section to allocate huge pages.
<3> Set the `match` section to associate the profile to nodes with the `hugepages` label.

. Create the custom `hugepages` tuned profile by using the `hugepages-tuned-runtime.yaml` file:
+
[source,terminal]
----
$ oc create -f hugepages-tuned-runtime.yaml
----

. After creating the profile, the Operator applies the new profile to the correct
node and allocates huge pages. Check the logs of a tuned pod on a node using
huge pages to verify:
+
[source,terminal]
----
$ oc logs <tuned_pod_on_node_using_hugepages> \
    -n openshift-cluster-node-tuning-operator | grep 'applied$' | tail -n1
----
+
----
2019-08-08 07:20:41,286 INFO     tuned.daemon.daemon: static tuning from profile 'node-hugepages' applied
----

////
